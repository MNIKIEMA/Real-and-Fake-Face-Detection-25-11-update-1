{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from utils import FaceDataset\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5296)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = FaceDataset(\"data\")\n",
    "data.image_dict[\"label_bin\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9*len(data))\n",
    "train_data, test_data = random_split(data, [train_size, len(data)-train_size])\n",
    "valid_size = int(0.1*len(train_data))\n",
    "train_data, valid_data = random_split(train_data, [len(train_data)-valid_size,\n",
    "                                                   valid_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(datas, train_size=0.8):\n",
    "    \n",
    "    \"\"\" Function to split data in training, valid and testing\n",
    "        Args:   data torch Dataset\n",
    "                train_size the training data size \"\"\"\n",
    "    training_size = int(train_size*len(datas))\n",
    "    train_data, test_data = random_split(datas, [training_size, len(datas)-training_size])\n",
    "    valid_size = int(0.1*len(train_data))\n",
    "    train_data, valid_data = random_split(train_data, [len(train_data)-valid_size,\n",
    "                                                   valid_size])\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(dataset= train_data,batch_size=100, shuffle=True, \n",
    "                               drop_last=True)\n",
    "valid_data_loader = DataLoader(dataset= valid_data,batch_size=10, shuffle=True, \n",
    "                               drop_last=True)\n",
    "test_data_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=True, \n",
    "                              drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([#transforms.Resize((30,30)),\n",
    "    #transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    #transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceDataset(\"data\", transform=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = torch.utils.data.ConcatDataset([data, dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train, aug_valid, aug_test = split_data(data_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k = DataLoader(dataset= data_aug,batch_size=10, shuffle=True, \n",
    "                               drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_loader = DataLoader(dataset= aug_train,batch_size=10, shuffle=True, \n",
    "                               drop_last=True)\n",
    "valid_aug_loader = DataLoader(dataset= aug_valid,batch_size=10, shuffle=True, \n",
    "                               drop_last=True)\n",
    "test_aug_loader = DataLoader(dataset=aug_test, batch_size=10, shuffle=True, \n",
    "                              drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 600, 600])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_k))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, act_fn):\n",
    "        super(MLPClassif, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        dropout = nn.Dropout(p=0.1)\n",
    "        x = self.act_fn(self.hidden1(x))\n",
    "        x = nn.BatchNorm1d(100, affine=False)(x)\n",
    "        x = self.act_fn(self.hidden2(x))\n",
    "        x = nn.BatchNorm1d(100, affine=False)(x)\n",
    "        x = self.act_fn(self.hidden3(x))\n",
    "        x = nn.BatchNorm1d(100, affine=False)(x)\n",
    "        out = self.out_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassif(30*30, 100, 1, nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassif(\n",
       "  (hidden1): Linear(in_features=900, out_features=100, bias=True)\n",
       "  (hidden2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (hidden3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (out_layer): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (act_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "\n",
    "#torch.manual_seed(0)\n",
    "model.apply(init_weights)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_binary_classifier(model, eval_dataloader, loss_fn):\n",
    "    #model.to(device)\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "    #model.to(device)\n",
    "    transform = transforms.Resize(size = (30,30))\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        loss = 0\n",
    "\n",
    "        \n",
    "\n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "            #labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            images = transform(images.squeeze(0))\n",
    "            \n",
    "            #labels.to(device)\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            #images.to(device)\n",
    "            #print(images.shape)\n",
    "            # Get the predicted labels\n",
    "            y_predicted = model(images)\n",
    "\n",
    "            l =loss_fn(y_predicted, labels.unsqueeze(1))\n",
    "            loss += l.item()\n",
    "            \n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_val_binary_classifier(model, train_dataloader, valid_dataloader, num_epochs, \n",
    "                            loss_fn, learning_rate, verbose=True):\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "    #model_tr.to(device)\n",
    "    # Define the optimizer\n",
    "    #optimizer = #torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    optimizer =torch.optim.SGD(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    valid_loss = []\n",
    "    min_valid_loss = 0\n",
    "    transform = transforms.Resize(size = (30,30))\n",
    "    # Training loop\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            #labels = torch.tensor(labels, dtype =torch.float32)\n",
    "            #images.to(device)\n",
    "            #images = torch.tensor(images, dtype=float)\n",
    "            images = transform(images.squeeze(0))\n",
    "            #labels = torch.Tensor([(l==1) for l in labels])\n",
    "            #labels.to(device)\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            \n",
    "            \n",
    "            y_pred = model_tr(images)\n",
    "            l = loss_fn(y_pred, labels.unsqueeze(1))\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_current_epoch +=  l.item()\n",
    "\n",
    "\n",
    "\n",
    "        # At the end of each epoch, record and display the loss over all batches\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        val_loss= eval_binary_classifier(model_tr, \n",
    "                                                 valid_dataloader,\n",
    "                                                 loss_fn)\n",
    "        valid_loss.append(val_loss)\n",
    "\n",
    "        #if val_loss < min_valid_loss:\n",
    "        #    min_valid_loss = val_loss\n",
    "        #    torch.save(model_tr.state_dict(), 'model_opt.pt')\n",
    "                                    \n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Train Loss: {:.4f}\\n\\\n",
    "                Valid Loss: {:.4f}'.format(epoch+1, num_epochs, \n",
    "                                           loss_current_epoch,\n",
    "                                           val_loss))\n",
    "\n",
    "            \n",
    "        \n",
    "    return model_tr, loss_all_epochs, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "learning_rate = 0.08\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model_trained, train_losses, val_losses =train_val_binary_classifier(model, \n",
    "                                                                     train_aug_loader,\n",
    "                                                                     valid_aug_loader,\n",
    "                                                                     num_epochs,loss_fn, \n",
    "                                                                     learning_rate, \n",
    "                                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataAugmentation(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, transform = None):\n",
    "        \n",
    "        \"\"\"Function to load images into Tensor\n",
    "            Args: \n",
    "                - image_dir : directory of images\n",
    "                - Return : a dictonary with images and labels\n",
    "                \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_dict = self.load_image()\n",
    "\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.image_dict[\"label\"])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        \n",
    "        \n",
    "        img = self.image_dict[\"img_dir\"][index]\n",
    "        path = torch.from_numpy(io.imread(img,\n",
    "                                         as_gray=True).astype(np.float32))\n",
    "        label = self.image_dict[\"label_bin\"][index]\n",
    "        if self.transform is not None:\n",
    "            for t in self.transform:\n",
    "                path = t(path)\n",
    "        \n",
    "        return path, label\n",
    "\n",
    "\n",
    "    def load_image(self) :\n",
    "        img_dict = {\"img_dir\" : [], \"label\" : [], 'label_bin':[]}\n",
    "        for root, dirs, files in os.walk(self.image_dir):\n",
    "            for img in files:\n",
    "                img_dir = os.path.join(root, img)\n",
    "                \n",
    "                img_dict[\"img_dir\"].append(img_dir)\n",
    "                img_dict[\"label\"].append(img[:4])\n",
    "                img_dict[\"label_bin\"].append(float(img[:4]==\"real\"))\n",
    "        img_dict[\"label_bin\"] = torch.tensor(img_dict[\"label_bin\"],dtype=torch.float32)\n",
    "        return img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 30, 30])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_ = transforms.Resize(size = (30,30))\n",
    "img.shape\n",
    "img = transform_(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "augdata = FaceDataAugmentation('data', transform = [transforms.RandomHorizontalFlip(),transforms.Resize(size = (24,24))])\n",
    "augdataloader = DataLoader(dataset= augdata,batch_size=10, shuffle=True, \n",
    "                               drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(augdataloader))[0][0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses over epochs\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_losses)\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(model, eval_dataloader):\n",
    "\n",
    "    logits = nn.Sigmoid()\n",
    "    model.eval() \n",
    "    transform = transforms.Resize(size = (30,30))\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in eval_dataloader:\n",
    "            #images = torch.tensor(images, dtype=float)\n",
    "            images = transform(images)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            \n",
    "            #label.to(device)\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            y_predicted = model(images)\n",
    "            y_predicted = logits(y_predicted)\n",
    "            # print(y_predicted)\n",
    "            label_predicted = torch.round(y_predicted)\n",
    "            #print(label_predicted)\n",
    "            total += labels.size(0)\n",
    "            correct += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.5"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_ =  MLPClassif(30*30, 32, 1, nn.ReLU())\n",
    "#model_.load_state_dict(torch.load('model_opt.pt'))\n",
    "torch.save(model_trained, 'model_opt.pt')\n",
    "eval_classifier(model_trained, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels1=16, num_channels2=32, num_classes=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels= num_channels1, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           #nn.BatchNorm2d(num_channels1, affine=False),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           #nn.BatchNorm2d(num_channels2,affine=False),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        self.fc = nn.Linear(32*7*7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        out = self.fc(x.view(-1, 32*7*7))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 0.08\n",
    "\n",
    "cv = nn.Sequential(nn.Conv2d(in_channels=1,out_channels=16, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(16, affine=False),\n",
    "                           nn.MaxPool2d(kernel_size=2),\n",
    "                           )\n",
    "print(img.unsqueeze(1).shape)\n",
    "out = cv(img.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 15, 15])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 15, 15]) torch.Size([10, 32, 7, 7])\n",
      "torch.Size([1, 15680])\n"
     ]
    }
   ],
   "source": [
    "conv_block2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "out2 = conv_block2(out)\n",
    "print(out.shape, out2.shape)\n",
    "print(out2.view(-1,15680).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6009, -0.2867],\n",
       "        [ 0.1835,  0.6055],\n",
       "        [-0.5437, -0.3293]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 2)\n",
    "y = torch.randn(3, 2)\n",
    "z = torch.stack([x,y, torch.randn(3, 2)], dim=0)\n",
    "z[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 114.5117\n",
      "Epoch [2/30], Loss: 113.5412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb Cellule 33\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss_current_epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Iterate over batches using the dataloader\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_index, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m# TO DO: write the training procedure for each batch. This should consist of:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m# - vectorizing the images (size should be (batch_size, input_size))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m# - apply the forward pass (calculate the predicted labels from the vectorized images)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m# - use the 'backward' method to compute the gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m# - apply the gradient descent algorithm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m# Also think of updating the loss at the current epoch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# print(labels.unsqueeze(1).shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     images \u001b[39m=\u001b[39m transform_(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/face_.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model_cnn(images\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/Real-and-Fake-Face-Detection-25-11-update-1/utils.py:32\u001b[0m, in \u001b[0;36mFaceDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index) :\n\u001b[0;32m---> 32\u001b[0m     path \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(io\u001b[39m.\u001b[39;49mimread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_dict[\u001b[39m\"\u001b[39;49m\u001b[39mimg_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m][index],\n\u001b[1;32m     33\u001b[0m                                      as_gray\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     34\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dict[\u001b[39m\"\u001b[39m\u001b[39mlabel_bin\u001b[39m\u001b[39m\"\u001b[39m][index]\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m path, label\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/skimage/io/_io.py:48\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[0;32m---> 48\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39;49m\u001b[39mimread\u001b[39;49m\u001b[39m'\u001b[39;49m, fname, plugin\u001b[39m=\u001b[39;49mplugin, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mplugin_args)\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/skimage/io/manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    205\u001b[0m                            (plugin, kind))\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/skimage/io/_plugins/imageio_plugin.py:10\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/__init__.py:97\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m\"\"\"imread(uri, format=None, **kwargs)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39mReads an image from the specified file. Returns a numpy array, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     90\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStarting with ImageIO v3 the behavior of this function will switch to that of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m iio.v3.imread. To keep the current behavior (and make this warning disappear)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     95\u001b[0m )\n\u001b[0;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m imread_v2(uri, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/v2.py:200\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m imopen_args \u001b[39m=\u001b[39m decypher_format_arg(\u001b[39mformat\u001b[39m)\n\u001b[1;32m    198\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39;49m\u001b[39mri\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    201\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mread(index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/core/imopen.py:213\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     plugin_instance \u001b[39m=\u001b[39m candidate_plugin(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m InitializationError:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# file extension doesn't match file type\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/config/plugins.py:107\u001b[0m, in \u001b[0;36mPluginConfig.plugin_class.<locals>.partial_legacy_plugin\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartial_legacy_plugin\u001b[39m(request):\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m LegacyPlugin(request, legacy_plugin)\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/core/legacy_plugin_wrapper.py:80\u001b[0m, in \u001b[0;36mLegacyPlugin.__init__\u001b[0;34m(self, request, legacy_plugin)\u001b[0m\n\u001b[1;32m     74\u001b[0m source \u001b[39m=\u001b[39m (\n\u001b[1;32m     75\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m<bytes>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request\u001b[39m.\u001b[39mraw_uri, \u001b[39mbytes\u001b[39m)\n\u001b[1;32m     77\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request\u001b[39m.\u001b[39mraw_uri\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mio_mode \u001b[39m==\u001b[39m IOMode\u001b[39m.\u001b[39mread:\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format\u001b[39m.\u001b[39;49mcan_read(request):\n\u001b[1;32m     81\u001b[0m         \u001b[39mraise\u001b[39;00m InitializationError(\n\u001b[1;32m     82\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m can not read `\u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/core/format.py:241\u001b[0m, in \u001b[0;36mFormat.can_read\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcan_read\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[1;32m    237\u001b[0m     \u001b[39m\"\"\"can_read(request)\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[39m    Get whether this format can read data from the specified uri.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_can_read(request)\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/plugins/pillow_legacy.py:269\u001b[0m, in \u001b[0;36mPillowFormat._can_read\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    267\u001b[0m factory, accept \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mOPEN[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplugin_id]\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m accept:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mif\u001b[39;00m request\u001b[39m.\u001b[39;49mfirstbytes \u001b[39mand\u001b[39;00m accept(request\u001b[39m.\u001b[39mfirstbytes):\n\u001b[1;32m    270\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/core/request.py:605\u001b[0m, in \u001b[0;36mRequest.firstbytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\"\"\"The first 256 bytes of the file. These can be used to\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mparse the header to determine the file-format.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_firstbytes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_first_bytes()\n\u001b[1;32m    606\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_firstbytes\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/core/request.py:614\u001b[0m, in \u001b[0;36mRequest._read_first_bytes\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     \u001b[39m# Prepare\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 614\u001b[0m         f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_file()\n\u001b[1;32m    615\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIOError\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename):  \u001b[39m# A directory, e.g. for DICOM\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Master2/DeepLearning/dl-env/lib/python3.10/site-packages/imageio/core/request.py:492\u001b[0m, in \u001b[0;36mRequest.get_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 492\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    494\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_uri_type \u001b[39m==\u001b[39m URI_ZIPPED:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# Get the correct filename\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     filename, name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename_zip\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_cnn = CNN()\n",
    "#model_cnn.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "optimizer = torch.optim.SGD(model_cnn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to record the training loss over epochs\n",
    "loss_all_epochs = []\n",
    "    \n",
    "    # Training loop\n",
    "for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "    loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "    for batch_index, (images, labels) in enumerate(train_data_loader):\n",
    "            \n",
    "            # TO DO: write the training procedure for each batch. This should consist of:\n",
    "            # - vectorizing the images (size should be (batch_size, input_size))\n",
    "            # - apply the forward pass (calculate the predicted labels from the vectorized images)\n",
    "            # - use the 'backward' method to compute the gradients\n",
    "            # - apply the gradient descent algorithm\n",
    "            # Also think of updating the loss at the current epoch\n",
    "        # print(labels.unsqueeze(1).shape)\n",
    "        images = transform_(images)\n",
    "        y_pred = model_cnn(images.unsqueeze(1))\n",
    "        # print(y_pred.shape)\n",
    "        l = loss_fn(y_pred, labels.view(-1, 1))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        loss_current_epoch += l.item()\n",
    "        optimizer.step() #update parameters\n",
    "\n",
    "        # At the end of each epoch, record and display the loss over all batches\n",
    "    loss_all_epochs.append(loss_current_epoch)\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('dl-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e4653ef3f08bbcc46c48a23d8290ce23ff8798f7d85b3da9ffbf3074ca96bd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
